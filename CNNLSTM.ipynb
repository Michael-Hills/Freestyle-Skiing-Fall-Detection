{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    X, y = [], []\n",
    "    columns = ['Pelvis Accel Sensor X,mG', 'Pelvis Accel Sensor Y,mG',\n",
    "       'Pelvis Accel Sensor Z,mG', 'Pelvis Rot X,', 'Pelvis Rot Y,',    \n",
    "       'Pelvis Rot Z,']\n",
    "    # Iterate through each subdirectory (falls and nonFalls)\n",
    "    for subdir in os.listdir(directory):\n",
    "        # Ignore any files in the directory\n",
    "        if not os.path.isdir(os.path.join(directory, subdir)):\n",
    "            continue\n",
    "        # Iterate through each file in the subdirectory\n",
    "        for file in os.listdir(os.path.join(directory, subdir)):\n",
    "        \n",
    "            filepath = os.path.join(directory, subdir, file)\n",
    "            # Load the data from each CSV file\n",
    "            data = pd.read_csv(filepath,usecols=columns)\n",
    "            # Append data and labels to X and y lists\n",
    "            X.append(data.values)  # Assuming all columns are features\n",
    "            # Assign label based on subdirectory\n",
    "            y.append(1 if subdir == 'fall' else 0)  # Assuming 'falls' is class 1, and 'nonFalls' is class 0\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy,class_weights):\n",
    "    verbose, epochs, batch_size = 1, 4, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "\n",
    "    n_steps, n_length = 4, 75\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.3)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','recall','precision'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\n",
    "    preds = model.predict(testX, batch_size=batch_size, verbose=1)\n",
    "    return np.argmax(preds,axis=1)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjhil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 186ms/step - accuracy: 0.4974 - loss: 0.6988 - precision: 0.5037 - recall: 0.5044\n",
      "Epoch 2/4\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 194ms/step - accuracy: 0.7918 - loss: 0.5245 - precision: 0.7973 - recall: 0.7893\n",
      "Epoch 3/4\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 204ms/step - accuracy: 0.8288 - loss: 0.4877 - precision: 0.8249 - recall: 0.8313\n",
      "Epoch 4/4\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 211ms/step - accuracy: 0.8688 - loss: 0.4834 - precision: 0.8658 - recall: 0.8681\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step\n",
      "Accuracy:  0.8994413407821229\n",
      "Precision:  0.7058823529411765\n",
      "Recall:  0.75\n",
      "F1:  0.7272727272727273\n",
      "[[274  20]\n",
      " [ 16  48]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# Load training data\n",
    "trainX, trainy = load_dataset('./3S/train')\n",
    "\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
    "                                    classes = np.unique(trainy),\n",
    "                                    y = trainy)\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "trainy = to_categorical(trainy)\n",
    "\n",
    "\n",
    "# Load testing data\n",
    "testX, testy = load_dataset('./3S/test')\n",
    "testy = to_categorical(testy)\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 6))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 6))\n",
    "\n",
    "\n",
    "trainX = np.delete(trainX, 1, axis=1)\n",
    "testX =  np.delete(testX, 1, axis=1)\n",
    "\n",
    "\n",
    "true_labels = [np.argmax(label) for label in testy]\n",
    "preds = evaluate_model(trainX, trainy, testX, testy,class_weights)\n",
    "\n",
    "print(\"Accuracy: \",  accuracy_score(true_labels, preds))\n",
    "print(\"Precision: \", precision_score(true_labels, preds))\n",
    "print(\"Recall: \", recall_score(true_labels, preds))\n",
    "print(\"F1: \", f1_score(true_labels, preds))\n",
    "print(confusion_matrix(true_labels,preds))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
